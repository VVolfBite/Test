import streamlit as st
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
from datetime import datetime
import os

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="æœ¬åœ°èŠå¤©æœºå™¨äºº",
    page_icon="ğŸ¤–",
    layout="wide"
)

# åˆå§‹åŒ–session state
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "system", "content": "You are a friendly chatbot who always responds in the style of a pirate."}
    ]

if "current_chat" not in st.session_state:
    st.session_state["current_chat"] = None

if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = {}

# åˆå§‹åŒ–æ¨¡å‹å‚æ•°
if "temperature" not in st.session_state:
    st.session_state["temperature"] = 0.7
if "max_tokens" not in st.session_state:
    st.session_state["max_tokens"] = 256
if "top_p" not in st.session_state:
    st.session_state["top_p"] = 0.95
if "top_k" not in st.session_state:
    st.session_state["top_k"] = 50

# åˆ›å»ºä¿å­˜èŠå¤©è®°å½•çš„ç›®å½•
CHAT_HISTORY_DIR = "chat_history"
os.makedirs(CHAT_HISTORY_DIR, exist_ok=True)

def save_chat_history(chat_id, messages):
    """ä¿å­˜èŠå¤©è®°å½•åˆ°æ–‡ä»¶"""
    file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat_id}.json")
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(messages, f, ensure_ascii=False, indent=2)

def load_chat_history(chat_id):
    """ä»æ–‡ä»¶åŠ è½½èŠå¤©è®°å½•"""
    file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat_id}.json")
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return None

def get_chat_list():
    """è·å–æ‰€æœ‰èŠå¤©è®°å½•æ–‡ä»¶åˆ—è¡¨"""
    chats = []
    for file in os.listdir(CHAT_HISTORY_DIR):
        if file.endswith(".json"):
            chat_id = file[:-5]  # ç§»é™¤.jsonåç¼€
            file_path = os.path.join(CHAT_HISTORY_DIR, file)
            timestamp = os.path.getmtime(file_path)
            chats.append({
                "id": chat_id,
                "timestamp": timestamp,
                "date": datetime.fromtimestamp(timestamp).strftime("%Y-%m-%d %H:%M:%S")
            })
    return sorted(chats, key=lambda x: x["timestamp"], reverse=True)

# æ ‡é¢˜
st.title("ğŸ’¬ TinyLlama èŠå¤©æœºå™¨äºº")
st.caption("TinyLlama-1.1B-Chat-v1.0")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("æ¨¡å‹ä¿¡æ¯")
    st.markdown("""
    - æ¨¡å‹ï¼šTinyLlama-1.1B-Chat
    - ç‰ˆæœ¬ï¼šv1.0
    - ç±»å‹ï¼šæœ¬åœ°éƒ¨ç½²
    """)
    
    st.header("å‚æ•°è®¾ç½®")
    # æ¸©åº¦æ»‘å—
    temperature = st.slider(
        "æ¸©åº¦ (Temperature)",
        min_value=0.1,
        max_value=1.0,
        value=st.session_state["temperature"],
        step=0.1,
        help="æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ã€‚è¾ƒä½çš„å€¼ä½¿è¾“å‡ºæ›´ç¡®å®šï¼Œè¾ƒé«˜çš„å€¼ä½¿è¾“å‡ºæ›´éšæœºã€‚"
    )
    st.session_state["temperature"] = temperature
    
    # æœ€å¤§ç”Ÿæˆé•¿åº¦æ»‘å—
    max_tokens = st.slider(
        "æœ€å¤§ç”Ÿæˆé•¿åº¦ (Max Tokens)",
        min_value=64,
        max_value=512,
        value=st.session_state["max_tokens"],
        step=64,
        help="æ§åˆ¶ç”Ÿæˆå›å¤çš„æœ€å¤§é•¿åº¦ã€‚"
    )
    st.session_state["max_tokens"] = max_tokens
    
    # Top-p æ»‘å—
    top_p = st.slider(
        "Top-p",
        min_value=0.1,
        max_value=1.0,
        value=st.session_state["top_p"],
        step=0.05,
        help="æ§åˆ¶è¾“å‡ºçš„å¤šæ ·æ€§ã€‚è¾ƒä½çš„å€¼ä½¿è¾“å‡ºæ›´é›†ä¸­ï¼Œè¾ƒé«˜çš„å€¼ä½¿è¾“å‡ºæ›´å¤šæ ·ã€‚"
    )
    st.session_state["top_p"] = top_p
    
    # Top-k æ»‘å—
    top_k = st.slider(
        "Top-k",
        min_value=1,
        max_value=100,
        value=st.session_state["top_k"],
        step=1,
        help="æ§åˆ¶æ¯æ¬¡ç”Ÿæˆæ—¶è€ƒè™‘çš„å€™é€‰è¯æ•°é‡ã€‚"
    )
    st.session_state["top_k"] = top_k
    
    st.header("èŠå¤©ç®¡ç†")
    
    # æ–°å»ºèŠå¤©æŒ‰é’®
    if st.button("æ–°å»ºèŠå¤©"):
        chat_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        st.session_state["current_chat"] = chat_id
        st.session_state["messages"] = [
            {"role": "system", "content": "You are a friendly chatbot who always responds in the style of a pirate."}
        ]
        st.rerun()
    
    # æ˜¾ç¤ºèŠå¤©å†å²åˆ—è¡¨
    st.subheader("å†å²èŠå¤©")
    chats = get_chat_list()
    for chat in chats:
        col1, col2 = st.columns([3, 1])
        with col1:
            if st.button(f"ğŸ“ {chat['date']}", key=f"chat_{chat['id']}"):
                st.session_state["current_chat"] = chat["id"]
                loaded_messages = load_chat_history(chat["id"])
                if loaded_messages:
                    st.session_state["messages"] = loaded_messages
                st.rerun()
        with col2:
            if st.button("ğŸ—‘ï¸", key=f"delete_{chat['id']}"):
                file_path = os.path.join(CHAT_HISTORY_DIR, f"{chat['id']}.json")
                if os.path.exists(file_path):
                    os.remove(file_path)
                if st.session_state["current_chat"] == chat["id"]:
                    st.session_state["current_chat"] = None
                    st.session_state["messages"] = [
                        {"role": "system", "content": "You are a friendly chatbot who always responds in the style of a pirate."}
                    ]
                st.rerun()

# åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
    model = AutoModelForCausalLM.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
    return tokenizer, model

# åŠ è½½æ¨¡å‹
try:
    tokenizer, model = load_model()
except Exception as e:
    st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    st.stop()

# å±•ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    if msg["role"] != "system":
        st.chat_message(msg["role"]).write(msg["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)

    # æ„å»ºå¯¹è¯æ¨¡æ¿
    prompt_text = tokenizer.apply_chat_template(
        st.session_state.messages, tokenize=False, add_generation_prompt=True
    )
    inputs = tokenizer(prompt_text, return_tensors="pt").to(model.device)

    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            outputs = model.generate(
                **inputs,
                max_new_tokens=st.session_state["max_tokens"],
                do_sample=True,
                temperature=st.session_state["temperature"],
                top_k=st.session_state["top_k"],
                top_p=st.session_state["top_p"]
            )
            response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
            st.write(response)
            st.session_state["messages"].append({"role": "assistant", "content": response})
            
            # ä¿å­˜èŠå¤©è®°å½•
            if st.session_state["current_chat"]:
                save_chat_history(st.session_state["current_chat"], st.session_state["messages"])
            else:
                chat_id = datetime.now().strftime("%Y%m%d_%H%M%S")
                st.session_state["current_chat"] = chat_id
                save_chat_history(chat_id, st.session_state["messages"]) 